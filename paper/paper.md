---
title: 'SAMBA: A Trainable Segmentation Web-App with Smart Labelling'
tags:
  - Python
  - Javascript
  - materials
  - segmentation
  - machine learning
authors:
  - name: Ronan Docherty
    orcid: 0000-0002-7332-0924
    affiliation: 1 # (Multiple affiliations must be quoted)
  - name: Isaac Squires
    affiliation: 2
  - name: Antonis Vamvakeros
    affiliation: 2
  - name: Samuel J. Cooper
    affiliation: 2
    corresponding: true
    equal-contrib: false
affiliations:
 - name: Department of Materials, Imperial College London, London SW7 2DB
   index: 1
 - name: Dyson School of Design Engineering, Imperial College London, London SW7 2DB
   index: 2
date: 17 July 2023
bibliography: paper.bib

---

# Summary

Segmentation is the assigning of a semantic class to every pixel in an image and is a prerequisite for downstream analysis in materials science, like phase quantification or morphological characterization. The wide range of length scales, imaging techniques and materials studied in materials science means any segmentation algorithm must generalise to unseen data and support abstract, user-defined semantic classes. 'Trainable segmentation' is a popular interactive segmentation paradigm where a random forest is trained to map from image features to user drawn labels. `SAMBA` is a trainable segmentation tool that uses Meta's Segment Anything Model for fast, high-quality label suggestions and random forests for robust, generalizable segmentations. It is accessible in the browser ([https://www.sambasegment.com/](https://www.sambasegment.com/)), without the need to download any external dependencies. 

# Statement of need

Trainable segmentation tools like ilastik [@ilastik] and Trainable Weka Segmentation [@weka] are popular options for image segmentation in biology and materials science due to their flexibility. Both apply common image processing operations like blurs, edge detection, and texture filters to create a feature stack, then train a classifier (usually a random forest ensemble) to map from the stack to user drawn labels on a pixelwise basis. ilastix offers fast segmentations and a responsive 'live-view' via lazy-loading, whereas Trainable Weka Segmentation is part of and interoperates with the FIJI ImageJ library [@FIJI].

Well-placed, representative labels are important for segmentation accuracy, but tooling is limited to polygons and/or pixel brushes which can make labelling boundaries time-consuming. `SAMBA` (Segment Anything Model Based App) uses the recent object-detecting Segment Anything Model [@SAM] as a 'Smart Labelling' tool, which offers suggestions for associated objects as the cursor is moved which can then be added as labels. This 'Smart Labelling' can quickly pick out portions of the same phase and is resilient against noise, varying exposure, and certain types of artefacts *etc.* 

`SAMBA` is also unique in being fully web-based, meaning it can be used without downloading a large binary, installing libraries or requiring significant local computational hardware. An associated gallery page allows users to contribute to and view an open dataset of micrographs, labels, segmentations and experimental metadata. It is designed to be usable by researchers regardless of prior programming or image analysis experience. 

# Design and usage

![**(a)** screenshot of the SAMBA website, displaying the different labelling options including SAM powered 'Smart Labelling'. **(b)** shows how changing the Smart Label region sizes affects the suggested label at the same mouse position (red), giving the user the flexibility to focus on different length scales. **(c)** an example output segmentation of the tool, which can be saved as `.tiff` for later analysis.  \label{fig:gui}](gui.png)

`SAM` is a *promptable*, open-source macroscopic object segmentation model. It feeds a prompt (click, bounding box, mask or text) alongside an embedding of an image generated by a pretrained autoencoder [@ViT-22B] through a lightweight decoder network to produce a segmentation for the prompt (*i.e,* the object at the mouse cursor). The decoder is lightweight enough that it can be run in the browser in real-time when exported via the `ONNX` framework [@ONNX], though the embedding must still be generated by a large autoencoder model running on a computer with `PyTorch`. Despite its impressive zero-shot performance, `SAM` is primarily an object-detection model and cannot be used for the semantic phase segmentation desired in materials science. However, its understanding of shape, texture and other lower-level features apply well to instances of certain phases.

This provides the motivation for and structure of `SAMBA`: a web server supplies the image embedding (and later performs random forest segmentation) for the user's image, then browser-based labelling is done by using client-side object segmentations as label suggestions. Labels are confirmed with a left click, and a right click switches length scales of the smart labelling - this was achieved by exposing `SAM`'s hidden multi-mask output. Normally, `SAM` produces three masks per prompt - each representing a different length scale and with an associated internal quality estimate - then returns the best one. Giving the user the choice of length scales allows greater flexibility, which is useful given `SAM` is not designed for micrographs so its internal estimate of quality may not always match the user's.

Standard drawing tools like a variable width brush, polygon, and eraser allow the user to create labels when `SAM` struggles, for example with very high frequency spatial features. Previous labels are not overwritten (unless erased), so new labels can be added on top for accurate boundaries or rapid whole-image labelling. These labels can then be downloaded for use in different, more involved workflows. One possible use of `SAMBA` is as a platform to rapidly create high-quality manual labels for materials machine learning applications, normally an onerous task and doubly so when qualified labellers are scarce.

After labelling is complete, the web server trains a random forest ensemble to map the feature stack (computed in the background) to the labels. The feature stack creation involves a Python reimplementation of most of the image processing operations available to Trainable Weka Segmentation, achieved predominantly using the `scikit-image` library [@scikit-image]. Once completed, the result is returned to the user as a variable opacity overlay, allowing for additional labels to refine the output. The trained classifier is a `scikit-learn` object [@scikit-learn] - it can be downloaded and applied to future data either on `SAMBA` or in Python.

`SAMBA` has a clean, responsive user interface implemented in React and Typescript. It works for large `.tiff` images as well as `.tiff` stacks. Local hosting is simple and advisable if the user is handling sensitive data or wishes to benefit from GPU acceleration. Another component of `SAMBA` is the gallery: once a segmentation is complete the user can choose to share their data to an open dataset and gallery page. Ideally this will produce a varied, open source of high-quality image data and encourage further development of generalist machine learning models in materials science. A user manual, instructions for contributing and setup instructions for local hosting is available in this [repository](https://github.com/tldr-group/samba-web).

# Acknowledgements

This work was supported by funding from the EPSRC Faraday Institution Multi-Scale Modelling project ([https://faraday.ac.uk/](https://faraday.ac.uk/) EP/S003053/1, grant number FIRG003 received by SJC), the EPSRC Centre for Doctoral Training in Advanced Characterisation of Materials (EP/S023259/1 recieved by RD) and the Royal Society (IF\\R2\\222059 received by AV as a Royal Society Industry Fellow).

The authors would like to thank other members of the TLDR group for their testing and feedback.

# References
